{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b433c32",
   "metadata": {},
   "source": [
    "#  Classification in Machine Learning\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Classification is one of the fundamental tasks in supervised machine learning, where the goal is to predict categorical labels for data points based on their features. Unlike regression, which predicts continuous values, classification assigns discrete class labels to instances.\n",
    "\n",
    "### Common Classification Problems\n",
    "\n",
    "Classification problems are everywhere in real-world applications:\n",
    "- **Email filtering systems** classify messages as spam or legitimate\n",
    "- **Medical diagnosis tools** predict whether a patient has a particular disease\n",
    "- **Credit card companies** detect fraudulent transactions\n",
    "- **E-commerce platforms** predict whether customers will churn or remain active\n",
    "- **Image recognition systems** classify objects in photographs\n",
    "\n",
    "**In business, \"churn\" is the rate at which a company loses customers or subscribers over a specific period**\n",
    "#Ech of these scenarios involves learning patterns from historical labeled data to make predictions on new, unseen instances.\n",
    "\n",
    "### Binary vs Multiclass Classification\n",
    "\n",
    "**Binary classification** involves predicting one of two possible classes, such as yes/no, true/false, or positive/negative. This is the most common type of classification problem. Examples include:\n",
    "- Predicting customer churn (will churn or won't churn)\n",
    "- Spam detection (spam or not spam)\n",
    "- Disease diagnosis (disease present or absent)\n",
    "\n",
    "**Multiclass classification** extends this to three or more classes. For instance:\n",
    "- Classifying images of animals into categories like cat, dog, bird, or fish\n",
    "- Handwritten digit recognition involves classifying digits 0 through 9\n",
    "- Product categorization in e-commerce might involve dozens or hundreds of categories\n",
    "\n",
    "### The Classification Workflow\n",
    "\n",
    "\n",
    "A typical classification project follows several key stages:\n",
    "1. Prepare and clean the data\n",
    "2. Establish a validation framework to properly evaluate models\n",
    "3. Perform EDA understand patterns\n",
    "4. Assess feature importance using various techniques\n",
    "5. Engineer new features and encode categorical variables\n",
    "6. Train classification models\n",
    "7. Interpret model behavior\n",
    "8. Deploy for predictions\n",
    "\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "### Handling Missing Values\n",
    "- Identify which features contain missing values and assess the extent of missingness\n",
    "- Decide on an appropriate strategy: remove rows with missing values, impute with mean/median/mode, or use advanced imputation techniques\n",
    "- Document which approach was used for reproducibility\n",
    "- Consider the impact of missing data patterns on model performance\n",
    "\n",
    "### Data Cleaning\n",
    "- Remove duplicate records that could bias the model\n",
    "- Fix inconsistencies in categorical values (e.g., \"yes\", \"Yes\", \"YES\" should be standardized)\n",
    "- Correct obvious data entry errors and anomalies\n",
    "- Ensure data types are appropriate for each feature\n",
    "- Validate that values fall within expected ranges\n",
    "\n",
    "### Data Formatting\n",
    "- Convert string representations to appropriate types (dates, numbers, categories)\n",
    "- Parse complex fields into usable components\n",
    "- Standardize text fields (lowercase, remove special characters)\n",
    "- Ensure consistent units of measurement across all records\n",
    "- Handle special characters and encoding issues\n",
    "\n",
    "### Outlier Treatment\n",
    "- Identify outliers using statistical methods (IQR, z-scores) or visualization\n",
    "- Determine if outliers are errors (remove) or genuine extreme values (keep)\n",
    "- Consider capping extreme values or transforming features to reduce outlier impact\n",
    "- Document decisions about outlier handling\n",
    "- Be cautious not to remove legitimate data points\n",
    "\n",
    "### Feature Scaling\n",
    "- Some algorithms (like logistic regression) benefit from normalized or standardized features\n",
    "- **Standardization** transforms features to have mean 0 and standard deviation 1\n",
    "- **Normalization** scales features to a fixed range, typically [0, 1]\n",
    "- Note: Tree-based models don't require feature scaling\n",
    "- Always fit scalers on training data only, then transform test data\n",
    "\n",
    "\n",
    "## Setting Up Validation Framework\n",
    "\n",
    "A proper validation framework is crucial for honestly assessing model performance and avoiding overfitting.\n",
    "\n",
    "### Data Splitting Strategy\n",
    "- Divide the dataset into three distinct subsets: **training, validation, and test sets**\n",
    "- 60-20-20  (train-validation-test)\n",
    "- Ensure the split is random to avoid introducing bias\n",
    "- For time-series data, use temporal splits to respect time ordering\n",
    "- Use `train_test_split` from scikit-learn with a fixed random_state for reproducibility\n",
    "\n",
    "### Purpose of Each Set\n",
    "\n",
    "**Training Set**\n",
    "- Used to fit the model and learn feature-target relationships\n",
    "- The model sees this data during the learning process\n",
    "- Typically the largest subset (60-80% of data)\n",
    "\n",
    "**Validation Set**\n",
    "- Used during model development to tune hyperparameters\n",
    "- Compare different models and select the best one\n",
    "- Never used for training the model\n",
    "- Helps detect overfitting\n",
    "\n",
    "**Test Set**\n",
    "- Held out until the very end to provide an unbiased estimate of final model performance\n",
    "- Never used for any decision-making during model development\n",
    "- Provides the final performance metric you report\n",
    "- Simulates real-world performance on completely unseen data\n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "**K-Fold Cross-Validation**\n",
    "- Splits training data into k subsets (folds), typically k=5 or k=10\n",
    "- The model is trained k times, each time using k-1 folds for training and 1 fold for validation\n",
    "- Results are averaged across all folds to get a more robust performance estimate\n",
    "- Particularly useful when you have limited data\n",
    "- Provides better estimate of model performance than single train-validation split\n",
    "\n",
    "**Stratified K-Fold**\n",
    "- Maintains class proportions in each fold\n",
    "- Important for imbalanced datasets\n",
    "- Ensures each fold is representative of the overall class distribution\n",
    "- Reduces variance in performance estimates\n",
    "\n",
    "### Handling Class Imbalance\n",
    "- Check if your target classes are balanced or imbalanced\n",
    "- Use stratified splitting to ensure each subset has representative class proportions\n",
    "- Consider oversampling minority class, undersampling majority class, or using SMOTE\n",
    "- Adjust evaluation metrics to account for imbalance (use precision, recall, F1 instead of just accuracy)\n",
    "- Use class_weight parameter in models to give more importance to minority class\n",
    "\n",
    "\n",
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA is the process of investigating your dataset to discover patterns, relationships, and potential issues before modeling.\n",
    "\n",
    "### Target Variable Analysis\n",
    "- Examine the distribution of the target variable (class balance)\n",
    "- Calculate the proportion of each class\n",
    "- Identify severe class imbalance that might require special handling\n",
    "- Visualize class distribution with bar charts or pie charts\n",
    "- Understanding baseline: what accuracy would you get by always predicting the majority class?\n",
    "\n",
    "### Feature Distributions\n",
    "\n",
    "**For Numerical Features**\n",
    "- Create histograms to understand their distributions\n",
    "- Use box plots to identify outliers and compare distributions across classes\n",
    "- Generate density plots to see smoothed distribution shapes\n",
    "- Check for skewness that might benefit from transformation\n",
    "- Look for bimodal or multimodal distributions\n",
    "\n",
    "**For Categorical Features**\n",
    "- Count unique values and their frequencies\n",
    "- Identify rare categories that might need grouping\n",
    "- Check for high cardinality features\n",
    "- Look for missing or unknown categories\n",
    "\n",
    "### Univariate Analysis\n",
    "- Examine each feature individually before looking at relationships\n",
    "- Calculate summary statistics (mean, median, std, min, max, quartiles)\n",
    "- Identify features with low variance that might not be useful\n",
    "- Look for features with excessive missing values\n",
    "- Understand the range and distribution of each feature\n",
    "\n",
    "### Bivariate Analysis\n",
    "- Plot features against the target variable to identify predictive relationships\n",
    "- Use grouped bar charts for categorical features vs target\n",
    "- Create box plots showing feature distributions for each class\n",
    "- Generate scatter plots for numerical features colored by target class\n",
    "- Look for clear separation between classes\n",
    "\n",
    "### Multivariate Analysis\n",
    "- Examine correlations between features using correlation matrices and heatmaps\n",
    "- Identify highly correlated features that might cause multicollinearity\n",
    "- Use pair plots to visualize multiple feature relationships simultaneously\n",
    "- Consider dimensionality reduction techniques (PCA) for high-dimensional data visualization\n",
    "- Look for feature interactions that might be important\n",
    "\n",
    "### Key Questions to Answer\n",
    "- Which features show clear separation between classes?\n",
    "- Are there any obvious patterns or trends?\n",
    "- Do any features have non-linear relationships with the target?\n",
    "- Are there interactions between features that might be important?\n",
    "- Is the data quality sufficient for modeling?\n",
    "- Are there any data quality issues that need addressing?\n",
    "\n",
    "\n",
    "## Feature Importance: Churn Rate and Risk Ratio\n",
    "\n",
    "Risk ratio is an intuitive way to measure how different values of a categorical feature relate to the target outcome, particularly useful in churn prediction problems.\n",
    "\n",
    "### Understanding Churn Rate\n",
    "- **Churn rate** is the proportion of customers who left (churned) within a specific group\n",
    "- Calculate it by dividing the number of churned customers by total customers in that group\n",
    "- Formula: `Churn Rate = (Number who churned) / (Total in group)`\n",
    "- Example: If 30 out of 100 customers with feature value A churned, the churn rate is 0.30 or 30%\n",
    "- This gives us a baseline understanding of churn within specific segments\n",
    "\n",
    "### Calculating Risk Ratio\n",
    "- **Risk ratio** compares a group's churn rate to the global average churn rate\n",
    "- Formula: `Risk Ratio = (Group Churn Rate) / (Global Churn Rate)`\n",
    "- Example: If global churn is 20% and a group has 40% churn, risk ratio = 40/20 = 2.0\n",
    "- This means the group is twice as likely to churn compared to average\n",
    "- Normalizes churn rates for easy comparison across different features\n",
    "\n",
    "### Interpreting Risk Ratio\n",
    "- **Risk ratio = 1**: The group has average risk (same as global churn rate)\n",
    "- **Risk ratio > 1**: Higher risk (feature value associated with more churn)\n",
    "- **Risk ratio < 1**: Lower risk (feature value associated with less churn)\n",
    "- **Risk ratio = 2**: Twice the risk\n",
    "- **Risk ratio = 0.5**: Half the risk\n",
    "\n",
    "### Practical Application\n",
    "- Calculate risk ratio for each category within categorical features\n",
    "- Identify high-risk segments that need intervention or special attention\n",
    "- Use risk ratios to create new binary features (high_risk vs low_risk)\n",
    "- Helps in feature selection by identifying which categorical features are most predictive\n",
    "- Provides business-friendly interpretation of feature importance\n",
    "\n",
    "### Example: Contract Type Analysis\n",
    "\n",
    "Suppose we're predicting customer churn and examining the \"contract_type\" feature:\n",
    "- Overall churn rate: 25%\n",
    "- Month-to-month contracts: 45% churn rate → Risk ratio = 45/25 = **1.8**\n",
    "- One-year contracts: 15% churn rate → Risk ratio = 15/25 = **0.6**\n",
    "- Two-year contracts: 5% churn rate → Risk ratio = 5/25 = **0.2**\n",
    "\n",
    "**Interpretation**: This clearly shows contract type is highly predictive, with month-to-month being high risk (1.8x normal risk), one-year being lower risk (0.6x), and two-year being very low risk (0.2x).\n",
    "\n",
    "\n",
    "## Feature Importance: Mutual Information\n",
    "\n",
    "Mutual information is a statistical measure that quantifies the dependency between a feature and the target variable.\n",
    "\n",
    "### What is Mutual Information?\n",
    "- Measures how much knowing the value of one variable reduces uncertainty about another variable\n",
    "- Based on information theory concepts (entropy)\n",
    "- Captures both **linear and non-linear** relationships between features and target\n",
    "- Higher mutual information scores indicate stronger relationships with the target\n",
    "- A score of 0 means the feature and target are completely independent\n",
    "\n",
    "### How It Works\n",
    "- Measures the reduction in entropy (uncertainty) of the target when we know the feature value\n",
    "- Unlike correlation, it can detect complex, non-linear dependencies\n",
    "- Works for both categorical and numerical features\n",
    "- No assumptions about the relationship between variables\n",
    "- Symmetric: MI(X, Y) = MI(Y, X)\n",
    "\n",
    "\n",
    "\n",
    "### Advantages of Mutual Information\n",
    "- Detects **non-linear relationships** that correlation might miss\n",
    "- Works with categorical features without requiring encoding\n",
    "- Provides a universal measure applicable to any feature type\n",
    "- Not sensitive to monotonic transformations of features\n",
    "- No assumptions about distribution of variables\n",
    "- Can handle mixed types of features\n",
    "\n",
    "### Limitations\n",
    "- Requires sufficient data to estimate accurately\n",
    "- Sensitive to feature discretization for continuous variables\n",
    "- Computationally more expensive than correlation\n",
    "- Doesn't indicate direction of relationship (positive or negative)\n",
    "\n",
    "### Practical Use\n",
    "- Rank features by mutual information scores for feature selection\n",
    "- Remove features with very low MI scores to reduce dimensionality\n",
    "- Compare MI scores across different feature sets\n",
    "- Use in combination with other importance measures for robust feature selection\n",
    "- Particularly useful when relationships are non-linear\n",
    "\n",
    "\n",
    "## Feature Importance: Correlation\n",
    "\n",
    "Correlation measures the linear relationship between numerical features and the target variable.\n",
    "\n",
    "### Understanding Correlation\n",
    "- **Correlation coefficient** quantifies the strength and direction of linear relationships\n",
    "- Values range from **-1** (perfect negative correlation) to **+1** (perfect positive correlation)\n",
    "- A value of **0** indicates no linear relationship\n",
    "- Most commonly uses **Pearson correlation coefficient**\n",
    "- Formula captures how two variables move together\n",
    "\n",
    "### Correlation with Binary Target\n",
    "- For binary classification (target encoded as 0 and 1), we can calculate correlation between features and target\n",
    "- **Positive correlation** means higher feature values are associated with the positive class (1)\n",
    "- **Negative correlation** means higher feature values are associated with the negative class (0)\n",
    "- The **magnitude** (absolute value) indicates strength of relationship\n",
    "- Can be calculated directly with pandas `.corr()` method\n",
    "\n",
    "### Interpreting Correlation Values\n",
    "\n",
    "**Strength Guidelines**\n",
    "- **|r| > 0.7**: Strong correlation\n",
    "- **0.3 < |r| ≤ 0.7**: Moderate correlation\n",
    "- **|r| ≤ 0.3**: Weak correlation\n",
    "\n",
    "**Direction**\n",
    "- **Positive (r > 0)**: Variables move in the same direction\n",
    "- **Negative (r < 0)**: Variables move in opposite directions\n",
    "- **r = 0**: No linear relationship\n",
    "\n",
    "### Limitations of Correlation\n",
    "- **Only captures linear relationships**: Misses non-linear patterns\n",
    "- **Sensitive to outliers**: Outliers can artificially inflate or deflate correlation\n",
    "- **Correlation ≠ causation**: Strong correlation doesn't imply one causes the other\n",
    "- **May be misleading** for categorical or ordinal features\n",
    "- Can't detect complex interactions between variables\n",
    "- Assumes both variables are continuous\n",
    "\n",
    "### Feature-Feature Correlation (Multicollinearity)\n",
    "- Examine correlations between features themselves\n",
    "- **Highly correlated features** (|r| > 0.8-0.9) provide redundant information\n",
    "- **Multicollinearity** can make model interpretation difficult\n",
    "- Consider removing one feature from highly correlated pairs\n",
    "- Helps reduce dimensionality without losing information\n",
    "- Use correlation heatmaps to visualize the correlation matrix\n",
    "\n",
    "### Using Correlation for Feature Selection\n",
    "- Select features with high absolute correlation with target\n",
    "- Remove redundant features with high inter-feature correlation\n",
    "- Use as one of several feature importance metrics\n",
    "- Combine with domain knowledge for better decisions\n",
    "- Be cautious about removing features that might have non-linear effects\n",
    "\n",
    "### Comparison with Other Methods\n",
    "- **Correlation**: Fast, simple, only linear relationships\n",
    "- **Mutual Information**: Captures non-linear, more computationally expensive\n",
    "- **Risk Ratio**: Intuitive for categorical features, business-friendly\n",
    "- Use multiple methods for robust feature selection\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is the process of creating new features or transforming existing ones to improve model performance.\n",
    "\n",
    "### Why Feature Engineering Matters\n",
    "- Good features can dramatically improve model accuracy\n",
    "- Domain knowledge is crucial for creating meaningful features\n",
    "- Often makes the difference between mediocre and excellent models\n",
    "- Can reveal hidden patterns not apparent in raw features\n",
    "- Sometimes more impactful than algorithm choice\n",
    "\n",
    "### Common Feature Engineering Techniques\n",
    "\n",
    "#### Creating Interaction Features\n",
    "- Combine two or more features through multiplication or other operations\n",
    "- Example: `total_price = quantity × unit_price`\n",
    "- Useful when the combination is more predictive than individual features\n",
    "- Can capture synergistic effects between variables\n",
    "- Example: `price_per_sqft = price / square_footage`\n",
    "\n",
    "#### Polynomial Features\n",
    "- Create squared, cubed, or other polynomial terms\n",
    "- Helps capture non-linear relationships\n",
    "- Example: `age²` might better capture age-related patterns\n",
    "- Be cautious of overfitting with high-degree polynomials\n",
    "- Example: `income²` for income effects that accelerate\n",
    "\n",
    "#### Binning Continuous Variables\n",
    "- Convert numerical features into categorical bins\n",
    "- Example: `age → age_group (young, middle-aged, senior)`\n",
    "- Can make relationships easier to interpret\n",
    "- Useful when relationships are non-linear or have thresholds\n",
    "- Example: `credit_score → credit_rating (poor, fair, good, excellent)`\n",
    "\n",
    "#### Date and Time Features\n",
    "- Extract components: `year, month, day, day_of_week, hour`\n",
    "- Calculate time differences: `days_since_last_purchase`\n",
    "- Create cyclical features for periodic patterns (day of week, month)\n",
    "- Identify special periods: `is_weekend, is_holiday, is_month_end`\n",
    "- Calculate duration: `account_age_days = current_date - registration_date`\n",
    "\n",
    "#### Aggregation Features\n",
    "- Calculate statistics across groups: `average_transaction_per_customer`\n",
    "- Count-based features: `number_of_purchases, number_of_logins`\n",
    "- Rolling window statistics: `30_day_average, moving_standard_deviation`\n",
    "- Useful in transactional or time-series data\n",
    "- Example: `total_spent_last_3_months, purchase_frequency`\n",
    "\n",
    "#### Text Features\n",
    "- Extract length of text: `comment_length, title_word_count`\n",
    "- Count specific characters or patterns: `num_exclamation_marks, num_urls`\n",
    "- Extract sentiment scores or keywords\n",
    "- Create TF-IDF or word embeddings for text classification\n",
    "- Boolean features: `contains_email, contains_phone_number`\n",
    "\n",
    "#### Ratio and Rate Features\n",
    "- Create ratios between related features\n",
    "- Example: `debt_to_income_ratio = total_debt / annual_income`\n",
    "- Example: `conversion_rate = conversions / visits`\n",
    "- Example: `average_order_value = total_revenue / number_orders`\n",
    "- Often more meaningful than absolute values\n",
    "\n",
    "#### Domain-Specific Features\n",
    "- Apply business or domain knowledge to create meaningful features\n",
    "- Example in credit scoring: `debt-to-income ratio, credit_utilization`\n",
    "- Example in e-commerce: `average_order_value, purchase_frequency, days_since_last_order`\n",
    "- Example in healthcare: `BMI = weight / height²`\n",
    "- These often become the most powerful predictors\n",
    "\n",
    "#### Feature Transformation\n",
    "- **Log transformation** for right-skewed features\n",
    "- **Square root** or **box-cox transformations**\n",
    "- **Normalize** or **standardize** features\n",
    "- Convert to categorical through binning or encoding\n",
    "- Apply domain-specific transformations\n",
    "\n",
    "\n",
    "## One-Hot Encoding\n",
    "\n",
    "Most machine learning algorithms require numerical inputs, but real-world data often contains categorical variables. One-hot encoding is the standard technique for converting categorical features into numerical format.\n",
    "\n",
    "### What is One-Hot Encoding?\n",
    "- Converts categorical variables into binary (0/1) dummy variables\n",
    "- Each unique category becomes a separate binary column\n",
    "- Only one column has value 1, all others have 0 (hence \"one-hot\")\n",
    "- Preserves categorical information without imposing ordinal relationships\n",
    "- Treats all categories as equally different (no inherent ordering)\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Suppose we have a feature \"color\" with values: red, blue, green\n",
    "\n",
    "**Original Data:**\n",
    "```\n",
    "Sample 1: color = \"red\"\n",
    "Sample 2: color = \"blue\"\n",
    "Sample 3: color = \"red\"\n",
    "Sample 4: color = \"green\"\n",
    "```\n",
    "\n",
    "**After One-Hot Encoding:**\n",
    "```\n",
    "Sample 1: color_red=1, color_blue=0, color_green=0\n",
    "Sample 2: color_red=0, color_blue=1, color_green=0\n",
    "Sample 3: color_red=1, color_blue=0, color_green=0\n",
    "Sample 4: color_red=0, color_blue=0, color_green=1\n",
    "```\n",
    "\n",
    "### Why One-Hot Encoding?\n",
    "\n",
    "**Problem with Label Encoding**\n",
    "- Simply converting categories to numbers (red=1, blue=2, green=3) implies ordering\n",
    "- Model might interpret blue (2) as \"between\" red (1) and green (3)\n",
    "- Creates artificial magnitude relationships\n",
    "- Not appropriate for nominal categories\n",
    "\n",
    "**One-Hot Encoding Solution**\n",
    "- No ordinal relationship is implied\n",
    "- Each category is treated independently\n",
    "- Model learns separate weights for each category\n",
    "- Appropriate for nominal categorical variables\n",
    "\n",
    "### Using DictVectorizer in Scikit-Learn\n",
    "\n",
    "DictVectorizer is an efficient way to implement one-hot encoding, particularly useful when data is in dictionary format.\n",
    "\n",
    "**Advantages of DictVectorizer:**\n",
    "- Converts dictionaries of feature-value pairs into numerical feature vectors\n",
    "- Automatically handles both categorical and numerical features\n",
    "- Efficient implementation with sparse matrices\n",
    "- Remembers feature names for interpretation\n",
    "- Single tool for mixed data types\n",
    "\n",
    "**Basic Usage:**\n",
    "```python\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Original data as list of dictionaries\n",
    "data = [\n",
    "    {'color': 'red', 'size': 'large', 'price': 100},\n",
    "    {'color': 'blue', 'size': 'small', 'price': 50},\n",
    "    {'color': 'green', 'size': 'medium', 'price': 75}\n",
    "]\n",
    "\n",
    "# Create and fit DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X = dv.fit_transform(data)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = dv.get_feature_names_out()\n",
    "print(feature_names)\n",
    "# Output: ['color=blue', 'color=green', 'color=red', 'price', 'size=large', 'size=medium', 'size=small']\n",
    "\n",
    "# X now contains one-hot encoded categorical features and original numerical features\n",
    "print(X)\n",
    "```\n",
    "\n",
    "**With Pandas DataFrames:**\n",
    "```python\n",
    "# Convert DataFrame rows to dictionaries\n",
    "train_dict = df_train.to_dict(orient='records')\n",
    "\n",
    "# Fit and transform\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "# Transform test data (use transform, not fit_transform)\n",
    "test_dict = df_test.to_dict(orient='records')\n",
    "X_test = dv.transform(test_dict)\n",
    "```\n",
    "\n",
    "### DictVectorizer vs OneHotEncoder\n",
    "\n",
    "**DictVectorizer:**\n",
    "- Works with dictionaries\n",
    "- Handles mixed types (categorical and numerical) automatically\n",
    "- Single step for all preprocessing\n",
    "- Convenient when data is already in dictionary format\n",
    "- Good for Kaggle-style tabular data\n",
    "\n",
    "**OneHotEncoder:**\n",
    "- Works with arrays\n",
    "- Requires separate preprocessing for different feature types\n",
    "- More control over encoding parameters\n",
    "- Part of sklearn.preprocessing\n",
    "- Better for pipelines with ColumnTransformer\n",
    "\n",
    "### Important Considerations\n",
    "\n",
    "**Dummy Variable Trap**\n",
    "- If you one-hot encode n categories, you only need n-1 columns\n",
    "- The nth column is redundant (can be inferred from others)\n",
    "- Can cause perfect multicollinearity in linear models\n",
    "- DictVectorizer keeps all columns but models handle this internally\n",
    "- Use `drop='first'` parameter in OneHotEncoder to avoid this\n",
    "\n",
    "**High Cardinality Features**\n",
    "- Features with many unique categories create many columns\n",
    "- Increases dimensionality significantly\n",
    "- Can lead to sparse data and overfitting\n",
    "- Consider alternative encodings for high-cardinality features\n",
    "\n",
    "**Unknown Categories at Test Time**\n",
    "- What if test data contains categories not seen in training?\n",
    "- DictVectorizer ignores unknown categories by default\n",
    "- OneHotEncoder can be set to handle unknown categories\n",
    "- Set `handle_unknown='ignore'` in OneHotEncoder\n",
    "\n",
    "**Memory Considerations**\n",
    "- One-hot encoding increases feature dimensionality\n",
    "- Use sparse matrices (`sparse=True`) to save memory\n",
    "- Sparse matrices store only non-zero values\n",
    "- Important for datasets with many categorical features\n",
    "\n",
    "### When to Use One-Hot Encoding\n",
    "- For nominal categorical features (no inherent order)\n",
    "- With tree-based models and linear models\n",
    "- When you have relatively few categories per feature (< 50)\n",
    "- Essential preprocessing step for most ML algorithms\n",
    "- Default choice for categorical encoding\n",
    "\n",
    "### Alternatives for High-Cardinality Features\n",
    "\n",
    "**Target Encoding (Mean Encoding)**\n",
    "- Replace category with mean target value for that category\n",
    "- Risk of overfitting; use cross-validation techniques\n",
    "- Reduces dimensionality significantly\n",
    "\n",
    "**Frequency Encoding**\n",
    "- Replace category with its frequency in the dataset\n",
    "- Simple and effective for some problems\n",
    "\n",
    "**Binary Encoding**\n",
    "- Convert categories to binary numbers\n",
    "- Fewer columns than one-hot encoding\n",
    "- Useful for high-cardinality features\n",
    "\n",
    "**Embedding Layers**\n",
    "- Learn dense representations for categories\n",
    "- Used in deep learning models\n",
    "- Captures relationships between categories\n",
    "\n",
    "### Complete Example\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'green', 'red', 'blue'],\n",
    "    'size': ['L', 'M', 'S', 'L', 'M'],\n",
    "    'price': [100, 50, 75, 120, 60],\n",
    "    'sold': [1, 0, 1, 1, 0]\n",
    "})\n",
    "\n",
    "# Split data\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare features and target\n",
    "X_train_dict = df_train[['color', 'size', 'price']].to_dict(orient='records')\n",
    "y_train = df_train['sold'].values\n",
    "\n",
    "X_test_dict = df_test[['color', 'size', 'price']].to_dict(orient='records')\n",
    "y_test = df_test['sold'].values\n",
    "\n",
    "# One-hot encode with DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(X_train_dict)\n",
    "X_test = dv.transform(X_test_dict)\n",
    "\n",
    "print(\"Feature names:\", dv.get_feature_names_out())\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"First sample encoded:\", X_train[0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Machine Learning for Classification\n",
    "\n",
    "With properly prepared and encoded data, we're ready to train classification models.\n",
    "\n",
    "### What is a Classification Model?\n",
    "- A classification model learns patterns from labeled training data\n",
    "- It creates a **decision boundary** that separates different classes\n",
    "- Once trained, it can predict class labels for new, unseen data\n",
    "- The model learns a function that maps input features to output classes\n",
    "- Training involves finding optimal parameters that minimize prediction errors\n",
    "\n",
    "### The Learning Process\n",
    "1. Feed the model training data with both features (X) and labels (y)\n",
    "2. The model adjusts its internal parameters to minimize prediction errors\n",
    "3. Different algorithms use different strategies to find optimal parameters\n",
    "4. The goal is to **generalize** well to new data, not just memorize training data\n",
    "5. Validation data helps ensure the model isn't overfitting\n",
    "\n",
    "### Types of Classification Algorithms\n",
    "\n",
    "**Linear Models**\n",
    "- Logistic Regression\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Simple, interpretable, fast\n",
    "- Work well when classes are linearly separable\n",
    "\n",
    "**Tree-Based Models**\n",
    "- Decision Trees: Simple, interpretable, handle non-linear relationships\n",
    "- Random Forests: Ensemble of trees, robust, high accuracy\n",
    "- Gradient Boosting (XGBoost, LightGBM, CatBoost): State-of-the-art performance\n",
    "- Handle non-linear relationships and interactions naturally\n",
    "\n",
    "**Instance-Based Learning**\n",
    "- K-Nearest Neighbors (KNN): Classifies based on similarity to nearby examples\n",
    "- Simple concept but can be slow with large datasets\n",
    "- No training phase, just stores data\n",
    "\n",
    "**Probabilistic Models**\n",
    "- Naive Bayes: Fast, works well with text data\n",
    "- Based on Bayes' theorem and conditional probability\n",
    "- Assumes feature independence (naive assumption)\n",
    "\n",
    "**Support Vector Machines (SVM)**\n",
    "- Finds optimal hyperplane to separate classes\n",
    "- Works well in high-dimensional spaces\n",
    "- Can use kernel trick for non-linear boundaries\n",
    "\n",
    "**Neural Networks and Deep Learning**\n",
    "- Can learn complex, hierarchical patterns\n",
    "- Requires more data and computational resources\n",
    "- Excellent for image, text, and complex structured data\n",
    "\n",
    "### Choosing a Classification Algorithm\n",
    "\n",
    "**Consider Dataset Size**\n",
    "- Small datasets (< 1000 samples): Logistic Regression, Naive Bayes, SVM\n",
    "- Medium datasets (1000-100K): Random Forest, Gradient Boosting\n",
    "- Large datasets (> 100K): Logistic Regression, Neural Networks, LightGBM\n",
    "\n",
    "**Feature Types**\n",
    "- Mixed categorical and numerical: Tree-based models (handle both natively)\n",
    "- Mostly numerical: Any algorithm, but consider scaling for linear models\n",
    "- High-dimensional: Logistic Regression, SVM, Neural Networks\n",
    "\n",
    "**Interpretability Needs**\n",
    "- High interpretability: Logistic Regression, Decision Trees\n",
    "- Moderate interpretability: Random Forest (feature importance)\n",
    "- Black box acceptable: Gradient Boosting, Neural Networks\n",
    "\n",
    "**Performance Requirements**\n",
    "- Fast training: Logistic Regression, Naive Bayes\n",
    "- Fast prediction: Logistic Regression, Linear models\n",
    "- Highest accuracy: Gradient Boosting, Neural Networks (with enough data)\n",
    "\n",
    "**General Strategy**\n",
    "- Start simple: Begin with Logistic Regression as baseline\n",
    "- Try tree-based: Random Forest or XGBoost for better performance\n",
    "- Experiment: Test multiple algorithms and compare\n",
    "- Ensemble: Combine multiple models for best results\n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is one of the most fundamental and widely-used classification algorithms, despite having \"regression\" in its name.\n",
    "\n",
    "### What is Logistic Regression?\n",
    "- A linear model designed specifically for classification tasks\n",
    "- Similar to linear regression but outputs probabilities instead of continuous values\n",
    "- Uses the logistic (sigmoid) function to transform linear combinations into probabilities\n",
    "- Despite the name, it's a **classification algorithm**, not a regression algorithm\n",
    "- Works by finding a linear decision boundary between classes\n",
    "\n",
    "### How It Works\n",
    "\n",
    "**Step 1: Linear Combination**\n",
    "- Like linear regression, computes a weighted sum of input features\n",
    "- Formula: `z = w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ`\n",
    "- w₀ is the intercept (bias term)\n",
    "- w₁, w₂, ..., wₙ are the weights (coefficients) for each feature\n",
    "- This gives us a score that can range from -∞ to +∞\n",
    "\n",
    "**Step 2: Sigmoid Transformation**\n",
    "- Instead of outputting z directly, passes it through the sigmoid function\n",
    "- Sigmoid function: `σ(z) = 1 / (1 + e^(-z))`\n",
    "- This transforms any real number into a value between 0 and 1\n",
    "- The output represents a probability\n",
    "\n",
    "**Step 3: Making Predictions**\n",
    "- Use a threshold (typically 0.5) to convert probability to class\n",
    "- If σ(z) ≥ 0.5, predict class 1\n",
    "- If σ(z) < 0.5, predict class 0\n",
    "\n",
    "### The Sigmoid Function\n",
    "\n",
    "**Mathematical Properties**\n",
    "- Domain: All real numbers (-∞ to +∞)\n",
    "- Range: (0, 1) - perfect for probabilities\n",
    "- When z = 0, sigmoid outputs 0.5\n",
    "- When z is large positive, sigmoid approaches 1\n",
    "- When z is large negative, sigmoid approaches 0\n",
    "- Creates an S-shaped curve\n",
    "\n",
    "**Visual Understanding**\n",
    "```\n",
    "σ(z)\n",
    "1.0 |           ________\n",
    "    |         /\n",
    "0.5 |       /\n",
    "    |     /\n",
    "0.0 |____/\n",
    "    |________________\n",
    "       -∞    0    +∞  (z)\n",
    "```\n",
    "\n",
    "**Why Sigmoid?**\n",
    "- Smoothly maps any real number to probability\n",
    "- Differentiable everywhere (important for gradient descent)\n",
    "- Has nice mathematical properties for optimization\n",
    "- Natural interpretation as probability\n",
    "\n",
    "### Output as Probability\n",
    "- The output represents **P(y=1|x)** - probability of positive class given features\n",
    "- For binary classification: P(y=1|x) = σ(z)\n",
    "- Probability of negative class: P(y=0|x) = 1 - σ(z)\n",
    "- These probabilities always sum to 1\n",
    "- Can use these probabilities for ranking or confidence scoring\n",
    "\n",
    "### Decision Boundary\n",
    "- Logistic regression creates a **linear decision boundary**\n",
    "- The boundary is where the probability equals 0.5 (where z = 0)\n",
    "- In 2D: boundary is a line; in 3D: a plane; in higher dimensions: a hyperplane\n",
    "- Points on one side → class 1; points on other side → class 0\n",
    "- The boundary equation: w₀ + w₁x₁ + w₂x₂ + ... + wₙxₙ = 0\n",
    "\n",
    "**Example in 2D:**\n",
    "```\n",
    "   x₂\n",
    "    |   Class 1 (σ > 0.5)\n",
    "    |  o  o  o\n",
    "    | o  o  o\n",
    "    |______________ Decision Boundary\n",
    "    |    x  x\n",
    "    | x  x  x\n",
    "    |  x  x    Class 0 (σ < 0.5)\n",
    "    |________________ x₁\n",
    "```\n",
    "\n",
    "### Comparison to Linear Regression\n",
    "\n",
    "**Similarities**\n",
    "- Both are linear models\n",
    "- Both use weighted sum of features: w₀ + w₁x₁ + ...\n",
    "- Both have interpretable weights\n",
    "- Both are simple and fast\n",
    "\n",
    "**Differences**\n",
    "- **Linear Regression**: Predicts continuous values directly (y = w₀ + w₁x₁ + ...)\n",
    "- **Logistic Regression**: Predicts probabilities using sigmoid(w₀ + w₁x₁ + ...)\n",
    "- **Linear Regression**: Output can be any value\n",
    "- **Logistic Regression**: Output is bounded [0, 1]\n",
    "- **Linear Regression**: Used for regression tasks\n",
    "- **Logistic Regression**: Used for classification tasks\n",
    "- **Loss Function**: Linear uses MSE, Logistic uses log loss (cross-entropy)\n",
    "\n",
    "### Learning the Weights\n",
    "\n",
    "**Optimization Goal**\n",
    "- Find weights that maximize the likelihood of correct classifications\n",
    "- Equivalent to minimizing the log loss (cross-entropy loss)\n",
    "- Log loss penalizes confident wrong predictions heavily\n",
    "\n",
    "**Log Loss (Cross-Entropy)**\n",
    "```\n",
    "Loss = -[y·log(p) + (1-y)·log(1-p)]\n",
    "```\n",
    "- When y=1 and p is close to 1: loss is small (good)\n",
    "- When y=1 and p is close to 0: loss is large (bad)\n",
    "- When y=0 and p is close to 0: loss is small (good)\n",
    "- When y=0 and p is close to 1: loss is large (bad)\n",
    "\n",
    "**Optimization Algorithms**\n",
    "- Gradient Descent: Iteratively adjust weights in direction that reduces loss\n",
    "- Stochastic Gradient Descent (SGD): Use mini-batches for efficiency\n",
    "- L-BFGS: Quasi-Newton method, efficient for medium-sized datasets\n",
    "- Newton-CG, SAG, SAGA: Other optimization algorithms available\n",
    "\n",
    "**Regularization**\n",
    "- L2 regularization (Ridge): Penalizes sum of squared weights\n",
    "- L1 regularization (Lasso): Penalizes sum of absolute weights, can produce sparse models\n",
    "- Prevents overfitting by discouraging large weights\n",
    "- Controlled by hyperparameter C (inverse regularization strength)\n",
    "\n",
    "### Advantages of Logistic Regression\n",
    "- **Simple and interpretable**: Easy to understand what the model learned\n",
    "- **Fast to train and predict**: Efficient even on large datasets\n",
    "- **Provides probabilities**: Not just class labels, but confidence scores\n",
    "- **Works well with linearly separable classes**: Strong baseline\n",
    "- **Requires less training data**: Compared to more complex models\n",
    "- **No hyperparameter tuning required** for basic usage\n",
    "- **Industry standard**: Well-tested and widely used\n",
    "- **Good for online learning**: Can update with new data incrementally\n",
    "\n",
    "### Limitations\n",
    "- **Linear decision boundary**: Can't model complex non-linear relationships\n",
    "- **May underperform** with highly non-linear data\n",
    "- **Requires feature engineering** for non-linear patterns\n",
    "- **Sensitive to outliers**: Extreme values can influence weights\n",
    "- **Sensitive to feature scale**: Benefits from standardization\n",
    "- **Assumes features are relatively independent**: Multicollinearity can be problematic\n",
    "- **Can underfit** if relationships are too complex\n",
    "\n",
    "### When to Use Logistic Regression\n",
    "- As a **baseline model** for any classification problem\n",
    "- When **interpretability** is important\n",
    "- When you have **linearly separable** or nearly separable classes\n",
    "- When you need **probability estimates**\n",
    "- For **high-dimensional data** (works well even with many features)\n",
    "- When training **speed** is important\n",
    "- For **real-time predictions** (very fast inference)\n",
    "- In **regulated industries** where model transparency is required\n",
    "\n",
    "---\n",
    "\n",
    "## Training Logistic Regression with Scikit-Learn\n",
    "\n",
    "Scikit-learn makes it straightforward to train logistic regression models with just a few lines of code.\n",
    "\n",
    "\n",
    "\n",
    "### Important Hyperparameters\n",
    "\n",
    "#### C (Regularization Strength)\n",
    "- **Inverse of regularization strength**: Smaller C = stronger regularization\n",
    "- **Default**: C=1.0\n",
    "- **Lower C** (e.g., 0.01, 0.1): Stronger regularization, simpler model, helps prevent overfitting\n",
    "- **Higher C** (e.g., 10, 100): Weaker regularization, more complex model, fits training data more closely\n",
    "- **Tuning**: Use cross-validation to find optimal value\n",
    "- **Range to try**: [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "\n",
    "#### penalty (Regularization Type)\n",
    "- **'l2'** (default): Ridge regularization - penalizes sum of squared weights\n",
    "- **'l1'**: Lasso regularization - penalizes sum of absolute weights, can produce sparse models\n",
    "- **'elasticnet'**: Combination of L1 and L2 (requires 'saga' solver)\n",
    "- **'none'**: No regularization (not recommended, can overfit)\n",
    "\n",
    "\n",
    "\n",
    "#### solver (Optimization Algorithm)\n",
    "- **'lbfgs'** (default): Good for most problems, supports L2 and no penalty\n",
    "- **'liblinear'**: Good for small datasets, supports L1 and L2\n",
    "- **'saga'**: Fast for large datasets, supports all penalties including elasticnet\n",
    "- **'sag'**: Stochastic Average Gradient, faster for large datasets\n",
    "- **'newton-cg'**: Good for small datasets with many features\n",
    "\n",
    "**Choosing a solver:**\n",
    "- Small dataset: 'liblinear' or 'lbfgs'\n",
    "- Large dataset: 'saga' or 'sag'\n",
    "- Need L1 regularization: 'liblinear' or 'saga'\n",
    "- Need elasticnet: 'saga'\n",
    "\n",
    "\n",
    "\n",
    "#### max_iter\n",
    "- **Maximum number of iterations** for optimization algorithm\n",
    "- **Default**: 100 (often insufficient)\n",
    "- If you get convergence warnings, increase this value\n",
    "- **Common values**: 1000, 5000, 10000\n",
    "- Higher values increase training time\n",
    "\n",
    "\n",
    "#### random_state\n",
    "- Sets random seed for reproducibility\n",
    "- Important when using stochastic solvers ('sag', 'saga')\n",
    "- Use any integer for consistent results\n",
    "\n",
    "\n",
    "\n",
    "## Model Interpretation\n",
    "\n",
    "Understanding what your model has learned is crucial for trust, debugging, and gaining business insights.\n",
    "\n",
    "### Examining Feature Coefficients (Weights)\n",
    "\n",
    "\n",
    "### Understanding Weight Values\n",
    "\n",
    "**Sign of Weights**\n",
    "- **Positive weights**: Increase the probability of the positive class\n",
    "- **Negative weights**: Decrease the probability of the positive class (increase negative class probability)\n",
    "- **Weight ≈ 0**: Feature doesn't affect predictions much\n",
    "\n",
    "**Magnitude of Weights**\n",
    "- **Larger absolute values**: Stronger influence on predictions\n",
    "- **Smaller absolute values**: Weaker influence\n",
    "- Compare magnitudes to identify most important features\n",
    "\n",
    "### Mathematical Interpretation\n",
    "\n",
    "**Log-Odds Interpretation**\n",
    "- Weights directly affect the log-odds of the positive class\n",
    "- Log-odds = log(p/(1-p)) where p is probability of positive class\n",
    "- One unit increase in feature changes log-odds by the weight value\n",
    "- This is before applying the sigmoid transformation\n",
    "\n",
    "**Example:**\n",
    "- Weight for `tenure` = -0.03\n",
    "- One month increase in tenure decreases log-odds of churn by 0.03\n",
    "- This translates to slightly lower probability of churning\n",
    "\n",
    "\n",
    "\n",
    "**Understanding Odds Ratios**\n",
    "- **Odds ratio = exp(weight)**\n",
    "- **Odds ratio > 1**: Feature increases odds of positive class\n",
    "- **Odds ratio < 1**: Feature decreases odds of positive class\n",
    "- **Odds ratio = 1**: Feature has no effect\n",
    "\n",
    "**Examples:**\n",
    "- Odds ratio = 2.0: Doubling the odds (100% increase)\n",
    "- Odds ratio = 1.5: 50% increase in odds\n",
    "- Odds ratio = 0.5: Halving the odds (50% decrease)\n",
    "- Odds ratio = 0.75: 25% decrease in odds\n",
    "\n",
    "\n",
    "\n",
    "### One-Hot Encoded Features\n",
    "\n",
    "**Interpreting Categorical Features**\n",
    "- Each category gets its own coefficient\n",
    "- Interpretation is relative to the reference category (usually the omitted one)\n",
    "- Compare coefficients within the same original feature\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "contract_month-to-month: +1.2\n",
    "contract_one-year: -0.3\n",
    "contract_two-year: -0.9\n",
    "\n",
    "Interpretation:\n",
    "- Month-to-month: Highest churn risk (positive, large weight)\n",
    "- One-year: Lower than month-to-month\n",
    "- Two-year: Lowest churn risk (negative, large magnitude)\n",
    "```\n",
    "\n",
    "### Limitations of Weight Interpretation\n",
    "\n",
    "**Scale Dependence**\n",
    "- Weights are affected by feature scale\n",
    "- A feature with larger scale will have smaller weight\n",
    "- **Solution**: Standardize features before training for fair comparison\n",
    "\n",
    "**Multicollinearity**\n",
    "- Highly correlated features can have unstable, misleading weights\n",
    "- Weight might be split between correlated features\n",
    "- **Solution**: Remove one feature from highly correlated pairs\n",
    "\n",
    "**Linear Effects Only**\n",
    "- Weights show linear effects, not interactions\n",
    "- Don't capture how features work together\n",
    "- **Solution**: Create interaction features or use tree-based models for interpretation\n",
    "\n",
    "**Reference Category**\n",
    "- One-hot encoded features interpreted relative to omitted category\n",
    "- Need to know which category was the reference\n",
    "\n",
    "### Business Insights from Interpretation\n",
    "\n",
    "**Identifying Key Drivers**\n",
    "- Which factors most strongly predict the outcome?\n",
    "- What characteristics define high-risk vs low-risk customers?\n",
    "\n",
    "**Validating Model**\n",
    "- Do the patterns make business sense?\n",
    "- Are the relationships aligned with domain knowledge?\n",
    "- Any surprising or suspicious patterns?\n",
    "\n",
    "**Actionable Insights**\n",
    "- Which factors can the business influence?\n",
    "- Where should interventions be focused?\n",
    "- What customer segments need attention?\n",
    "\n",
    "\n",
    "**Why Use Probabilities?**\n",
    "- Provides confidence level in predictions\n",
    "- Allows for ranking customers by risk\n",
    "- Enables custom business rules and thresholds\n",
    "- Better for decision-making than binary predictions\n",
    "\n",
    "### Using Custom Thresholds\n",
    "\n",
    "The default threshold is 0.5, but you can adjust based on business needs.\n",
    "\n",
    "**Why Adjust Thresholds?**\n",
    "- **Lower threshold** (e.g., 0.3): More sensitive, catches more at-risk customers, but more false alarms\n",
    "- **Higher threshold** (e.g., 0.7): More conservative, fewer false alarms, but might miss some at-risk customers\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**Feature Importance Methods**\n",
    "- **Risk Ratio**: Intuitive measure for categorical features comparing group churn rates to global rates\n",
    "- **Mutual Information**: Captures both linear and non-linear relationships between features and target\n",
    "- **Correlation**: Measures linear relationships; fast and simple but limited to linear patterns\n",
    "- Use multiple methods together for robust feature selection\n",
    "\n",
    "**Data Preparation and Validation**\n",
    "- Proper data cleaning and handling missing values is crucial\n",
    "- Split data into training, validation, and test sets\n",
    "- Use stratified splitting for imbalanced datasets\n",
    "- Cross-validation provides robust performance estimates\n",
    "\n",
    "**Exploratory Data Analysis**\n",
    "- Understand target distribution and check for class imbalance\n",
    "- Examine feature distributions and relationships with target\n",
    "- Identify patterns, outliers, and data quality issues\n",
    "- Guide feature engineering decisions with EDA insights\n",
    "\n",
    "**Feature Engineering**\n",
    "- Create interaction features, polynomial terms, and ratios\n",
    "- Extract components from dates and aggregate statistics\n",
    "- Apply domain knowledge for meaningful derived features\n",
    "- Significantly impacts model performance, often more than algorithm choice\n",
    "\n",
    "**One-Hot Encoding**\n",
    "- Essential for converting categorical variables to numerical format\n",
    "- **DictVectorizer** efficiently implements one-hot encoding\n",
    "- Works seamlessly with both categorical and numerical features in dictionary format\n",
    "- Each category becomes a separate binary column\n",
    "- Critical preprocessing step for most machine learning algorithms\n",
    "\n",
    "**Logistic Regression**\n",
    "- Linear model that outputs probabilities for classification\n",
    "- Uses sigmoid function to transform linear combination into probability [0, 1]\n",
    "- Despite \"regression\" in name, it's a classification algorithm\n",
    "- Similar to linear regression but designed for categorical outcomes\n",
    "- Creates linear decision boundary between classes\n",
    "\n",
    "**Training with Scikit-Learn**\n",
    "- Simple API: `model.fit(X_train, y_train)`\n",
    "- Key hyperparameters: C (regularization), penalty (L1/L2), solver, max_iter\n",
    "- Use `predict()` for class labels, `predict_proba()` for probabilities\n",
    "- Handle class imbalance with `class_weight='balanced'`\n",
    "- Cross-validation for hyperparameter tuning\n",
    "\n",
    "**Model Interpretation**\n",
    "- Examine feature weights to understand what model learned\n",
    "- Positive weights increase probability of positive class\n",
    "- Negative weights decrease probability of positive class\n",
    "- **Odds ratios** (exp(weight)) provide intuitive interpretation\n",
    "- Validate that learned patterns align with domain knowledge\n",
    "\n",
    "**Using the Model**\n",
    "- Always use same preprocessing on new data (transform, not fit_transform)\n",
    "- Get probability scores for risk-based decision making\n",
    "- Adjust prediction threshold based on business requirements\n",
    "- Lower threshold for sensitivity, higher threshold for precision\n",
    "- Create risk segments for targeted interventions\n",
    "\n",
    "**Best Practices**\n",
    "- Start with logistic regression as baseline\n",
    "- Feature engineering often more impactful than algorithm choice\n",
    "- Monitor model performance in production\n",
    "- Retrain periodically as data distribution changes\n",
    "- Document preprocessing and modeling decisions\n",
    "- Balance model complexity with interpretability needs\n",
    "\n",
    "### Workflow Recap\n",
    "\n",
    "1. **Data Preparation**: Clean data, handle missing values, split into train/validation/test\n",
    "2. **EDA**: Understand distributions, relationships, and identify issues\n",
    "3. **Feature Importance**: Use risk ratio, mutual information, and correlation to identify predictive features\n",
    "4. **Feature Engineering**: Create new features based on domain knowledge and EDA insights\n",
    "5. **Encoding**: Apply one-hot encoding to categorical features using DictVectorizer\n",
    "6. **Modeling**: Train logistic regression, tune hyperparameters\n",
    "7. **Interpretation**: Analyze feature weights and validate learned patterns\n",
    "8. **Deployment**: Use model for predictions with appropriate thresholds\n",
    "9. **Monitoring**: Track performance and update as needed\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "**For Balanced Datasets**\n",
    "- **Accuracy**: Proportion of correct predictions\n",
    "- Use when classes are roughly balanced\n",
    "\n",
    "**For Imbalanced Datasets**\n",
    "- **Precision**: Of predicted positives, how many are actually positive\n",
    "- **Recall**: Of actual positives, how many did we predict\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **AUC-ROC**: Area under ROC curve, threshold-independent metric\n",
    "\n",
    "**Business Metrics**\n",
    "- Cost of false positives vs false negatives\n",
    "- Coverage (what % of customers can we reach)\n",
    "- Lift (how much better than random)\n",
    "\n",
    "\n",
    "Classification with logistic regression provides a strong foundation for machine learning. The combination of proper data preparation, thoughtful feature engineering, and careful model interpretation creates models that are both accurate and trustworthy. The techniques covered here - from risk ratios to one-hot encoding to probability calibration - form the essential toolkit for tackling real-world classification problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23410a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Example: Customer Churn Feature Engineering\u001b[39;00m\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Time-based features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33maccount_age_months\u001b[39m\u001b[33m'\u001b[39m] = (pd.to_datetime(\u001b[33m'\u001b[39m\u001b[33mtoday\u001b[39m\u001b[33m'\u001b[39m) - \u001b[43mdf\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mregistration_date\u001b[39m\u001b[33m'\u001b[39m]).dt.days / \u001b[32m30\u001b[39m\n\u001b[32m     18\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mis_new_customer\u001b[39m\u001b[33m'\u001b[39m] = (df[\u001b[33m'\u001b[39m\u001b[33maccount_age_months\u001b[39m\u001b[33m'\u001b[39m] < \u001b[32m6\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Ratio features\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Classification in Machine Learning - Code Examples\n",
    "All code examples extracted from the classification notes\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE ENGINEERING EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Example: Customer Churn Feature Engineering\n",
    "\n",
    "# Time-based features\n",
    "df['account_age_months'] = (pd.to_datetime('today') - df['registration_date']).dt.days / 30\n",
    "df['is_new_customer'] = (df['account_age_months'] < 6).astype(int)\n",
    "\n",
    "# Ratio features\n",
    "df['charge_per_month'] = df['total_charges'] / df['tenure']\n",
    "df['service_ratio'] = df['num_services'] / df['max_possible_services']\n",
    "\n",
    "# Interaction features\n",
    "df['tech_support_senior'] = df['tech_support'] * df['is_senior_citizen']\n",
    "\n",
    "# Binning\n",
    "df['tenure_group'] = pd.cut(df['tenure'], \n",
    "                              bins=[0, 12, 24, 48, 100],\n",
    "                              labels=['0-1yr', '1-2yr', '2-4yr', '4+yr'])\n",
    "\n",
    "# Aggregation\n",
    "df['avg_monthly_charges'] = df.groupby('customer_id')['monthly_charges'].transform('mean')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CORRELATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate correlation with target\n",
    "correlation_with_target = df.corr()['target'].sort_values(ascending=False)\n",
    "print(correlation_with_target)\n",
    "\n",
    "# Full correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Visualize with heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify Highly Correlated Features\n",
    "def get_correlated_features(df, threshold=0.8):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper_triangle = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Find features with correlation > threshold\n",
    "    correlated_features = [\n",
    "        column for column in upper_triangle.columns \n",
    "        if any(upper_triangle[column] > threshold)\n",
    "    ]\n",
    "    \n",
    "    return correlated_features\n",
    "\n",
    "# Usage\n",
    "high_corr_features = get_correlated_features(df, threshold=0.85)\n",
    "print(f\"Highly correlated features: {high_corr_features}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MUTUAL INFORMATION\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Calculate mutual information scores\n",
    "mi_scores = mutual_info_classif(X_train, y_train)\n",
    "\n",
    "# Create a dataframe for better visualization\n",
    "mi_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'mi_score': mi_scores\n",
    "}).sort_values('mi_score', ascending=False)\n",
    "\n",
    "print(mi_df)\n",
    "\n",
    "# Calculate MI scores with visualization\n",
    "mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
    "\n",
    "# Create feature importance dataframe\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mi_score': mi_scores\n",
    "}).sort_values('mi_score', ascending=False)\n",
    "\n",
    "# Visualize top features\n",
    "top_n = 15\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['feature'][:top_n], \n",
    "         feature_importance['mi_score'][:top_n])\n",
    "plt.xlabel('Mutual Information Score')\n",
    "plt.title('Top Features by Mutual Information')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RISK RATIO CALCULATION\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_risk_ratio(df, feature, target):\n",
    "    \"\"\"Calculate risk ratio for a categorical feature\"\"\"\n",
    "    # Global churn rate\n",
    "    global_churn = df[target].mean()\n",
    "    \n",
    "    # Group churn rates\n",
    "    group_churn = df.groupby(feature)[target].mean()\n",
    "    \n",
    "    # Calculate risk ratios\n",
    "    risk_ratios = group_churn / global_churn\n",
    "    \n",
    "    return risk_ratios\n",
    "\n",
    "# Usage\n",
    "risk_ratios = calculate_risk_ratio(df, 'contract_type', 'churn')\n",
    "print(risk_ratios)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PREPARATION AND SPLITTING\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Convert to dictionaries if needed\n",
    "train_dict = X_train.to_dict(orient='records')\n",
    "test_dict = X_test.to_dict(orient='records')\n",
    "\n",
    "# Apply DictVectorizer for one-hot encoding\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train_encoded = dv.fit_transform(train_dict)\n",
    "X_test_encoded = dv.transform(test_dict)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ONE-HOT ENCODING WITH DICTVECTORIZER\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Original data as list of dictionaries\n",
    "data = [\n",
    "    {'color': 'red', 'size': 'large', 'price': 100},\n",
    "    {'color': 'blue', 'size': 'small', 'price': 50},\n",
    "    {'color': 'green', 'size': 'medium', 'price': 75}\n",
    "]\n",
    "\n",
    "# Create and fit DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X = dv.fit_transform(data)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = dv.get_feature_names_out()\n",
    "print(feature_names)\n",
    "\n",
    "# X now contains one-hot encoded categorical features and original numerical features\n",
    "print(X)\n",
    "\n",
    "# With Pandas DataFrames\n",
    "train_dict = df_train.to_dict(orient='records')\n",
    "\n",
    "# Fit and transform\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(train_dict)\n",
    "\n",
    "# Transform test data (use transform, not fit_transform)\n",
    "test_dict = df_test.to_dict(orient='records')\n",
    "X_test = dv.transform(test_dict)\n",
    "\n",
    "# Complete Example\n",
    "df = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'green', 'red', 'blue'],\n",
    "    'size': ['L', 'M', 'S', 'L', 'M'],\n",
    "    'price': [100, 50, 75, 120, 60],\n",
    "    'sold': [1, 0, 1, 1, 0]\n",
    "})\n",
    "\n",
    "# Split data\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare features and target\n",
    "X_train_dict = df_train[['color', 'size', 'price']].to_dict(orient='records')\n",
    "y_train = df_train['sold'].values\n",
    "\n",
    "X_test_dict = df_test[['color', 'size', 'price']].to_dict(orient='records')\n",
    "y_test = df_test['sold'].values\n",
    "\n",
    "# One-hot encode with DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train = dv.fit_transform(X_train_dict)\n",
    "X_test = dv.transform(X_test_dict)\n",
    "\n",
    "print(\"Feature names:\", dv.get_feature_names_out())\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"First sample encoded:\", X_train[0])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# LOGISTIC REGRESSION - BASIC TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Basic instantiation\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Or with specific parameters\n",
    "model = LogisticRegression(\n",
    "    C=1.0,              # Inverse regularization strength\n",
    "    max_iter=1000,      # Maximum iterations for convergence\n",
    "    solver='lbfgs',     # Optimization algorithm\n",
    "    random_state=42     # For reproducibility\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict class labels\n",
    "y_pred = model.predict(X_test_encoded)\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = model.predict_proba(X_test_encoded)\n",
    "# Returns array with [prob_class_0, prob_class_1] for each sample\n",
    "\n",
    "# Get probability for positive class only\n",
    "y_pred_proba_positive = y_pred_proba[:, 1]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HYPERPARAMETER TUNING\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_encoded, y_train)\n",
    "best_C = grid_search.best_params_['C']\n",
    "\n",
    "# L1 regularization (sparse model)\n",
    "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "# L2 regularization (default)\n",
    "model = LogisticRegression(penalty='l2', solver='lbfgs')\n",
    "\n",
    "# For large datasets with L1 penalty\n",
    "model = LogisticRegression(solver='saga', penalty='l1', max_iter=5000)\n",
    "\n",
    "# Increase if you see convergence warnings\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "\n",
    "# Automatic balancing\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# Custom weights\n",
    "model = LogisticRegression(class_weight={0: 1, 1: 3})  # Give 3x weight to class 1\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAKING PREDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Get hard class predictions (0 or 1)\n",
    "predictions = model.predict(X_test_encoded)\n",
    "\n",
    "# Get probability estimates\n",
    "probabilities = model.predict_proba(X_test_encoded)\n",
    "\n",
    "# Extract probability of positive class\n",
    "prob_positive = probabilities[:, 1]\n",
    "\n",
    "# Using Custom Thresholds\n",
    "proba_positive = model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "# Apply custom threshold (e.g., 0.7 for higher precision)\n",
    "threshold = 0.7\n",
    "custom_predictions = (proba_positive >= threshold).astype(int)\n",
    "\n",
    "# Or for lower threshold (higher recall)\n",
    "threshold = 0.3\n",
    "sensitive_predictions = (proba_positive >= threshold).astype(int)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "print(f\"AUC-ROC: {auc:.3f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPLETE EXAMPLE - CUSTOMER CHURN\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv('customer_churn.csv')\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('churn', axis=1)\n",
    "y = df['churn']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Convert to dictionaries\n",
    "train_dict = X_train.to_dict(orient='records')\n",
    "test_dict = X_test.to_dict(orient='records')\n",
    "\n",
    "# One-hot encode\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train_encoded = dv.fit_transform(train_dict)\n",
    "X_test_encoded = dv.transform(test_dict)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(\n",
    "    C=1.0,\n",
    "    max_iter=1000,\n",
    "    solver='lbfgs',\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_encoded)\n",
    "y_pred_proba = model.predict_proba(X_test_encoded)\n",
    "\n",
    "# Evaluate\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_names = dv.get_feature_names_out()\n",
    "weights = model.coef_[0]\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'weight': weights\n",
    "}).sort_values('weight', key=abs, ascending=False)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL PERSISTENCE (SAVING AND LOADING)\n",
    "# ============================================================================\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save model and vectorizer together\n",
    "with open('churn_model.pkl', 'wb') as f:\n",
    "    pickle.dump((dv, model), f)\n",
    "\n",
    "# Load model\n",
    "with open('churn_model.pkl', 'rb') as f:\n",
    "    dv_loaded, model_loaded = pickle.load(f)\n",
    "\n",
    "# Use loaded model for predictions\n",
    "new_customer = {'contract': 'month-to-month', 'tenure': 12}\n",
    "new_customer_encoded = dv_loaded.transform([new_customer])\n",
    "prediction = model_loaded.predict(new_customer_encoded)\n",
    "probability = model_loaded.predict_proba(new_customer_encoded)[0, 1]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CROSS-VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# 5-fold cross-validation\n",
    "cv_scores = cross_val_score(\n",
    "    model, \n",
    "    X_train_encoded, \n",
    "    y_train, \n",
    "    cv=5, \n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL INTERPRETATION\n",
    "# ============================================================================\n",
    "\n",
    "# Get weights (coefficients)\n",
    "weights = model.coef_[0]  # For binary classification\n",
    "\n",
    "# Get feature names\n",
    "feature_names = dv.get_feature_names_out()\n",
    "\n",
    "# Create interpretation dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'weight': weights,\n",
    "    'abs_weight': abs(weights)\n",
    "}).sort_values('abs_weight', ascending=False)\n",
    "\n",
    "print(importance_df.head(15))\n",
    "\n",
    "# Calculate odds ratios\n",
    "importance_df['odds_ratio'] = np.exp(importance_df['weight'])\n",
    "\n",
    "print(importance_df[['feature', 'weight', 'odds_ratio']].head(10))\n",
    "\n",
    "# Rank by absolute weight\n",
    "top_features = importance_df.sort_values('abs_weight', ascending=False).head(20)\n",
    "print(top_features[['feature', 'weight', 'odds_ratio']])\n",
    "\n",
    "# Positive influencers (increase churn)\n",
    "positive_features = importance_df[importance_df['weight'] > 0].sort_values('weight', ascending=False)\n",
    "print(\"\\nTop features increasing churn probability:\")\n",
    "print(positive_features.head(10))\n",
    "\n",
    "# Negative influencers (decrease churn)\n",
    "negative_features = importance_df[importance_df['weight'] < 0].sort_values('weight')\n",
    "print(\"\\nTop features decreasing churn probability:\")\n",
    "print(negative_features.head(10))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZING FEATURE IMPORTANCE\n",
    "# ============================================================================\n",
    "\n",
    "# Plot top features by absolute weight\n",
    "top_n = 15\n",
    "top_features = importance_df.sort_values('abs_weight', ascending=False).head(top_n)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red' if w < 0 else 'green' for w in top_features['weight']]\n",
    "plt.barh(range(len(top_features)), top_features['weight'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Weight (Coefficient)')\n",
    "plt.title('Top 15 Most Important Features')\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPLETE INTERPRETATION EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "# Full feature interpretation workflow\n",
    "weights = model.coef_[0]\n",
    "feature_names = dv.get_feature_names_out()\n",
    "\n",
    "# Create comprehensive interpretation dataframe\n",
    "interpretation = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'weight': weights,\n",
    "    'abs_weight': np.abs(weights),\n",
    "    'odds_ratio': np.exp(weights),\n",
    "    'pct_change_odds': (np.exp(weights) - 1) * 100\n",
    "}).sort_values('abs_weight', ascending=False)\n",
    "\n",
    "# Display top influential features\n",
    "print(\"=\" * 80)\n",
    "print(\"TOP 20 MOST INFLUENTIAL FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "print(interpretation.head(20).to_string(index=False))\n",
    "\n",
    "# Separate positive and negative influencers\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP FACTORS INCREASING CHURN RISK\")\n",
    "print(\"=\" * 80)\n",
    "positive = interpretation[interpretation['weight'] > 0].head(10)\n",
    "print(positive[['feature', 'odds_ratio', 'pct_change_odds']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP FACTORS DECREASING CHURN RISK\")\n",
    "print(\"=\" * 80)\n",
    "negative = interpretation[interpretation['weight'] < 0].head(10)\n",
    "print(negative[['feature', 'odds_ratio', 'pct_change_odds']].to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot 1: Top features by absolute weight\n",
    "top_15 = interpretation.head(15)\n",
    "colors = ['red' if w < 0 else 'green' for w in top_15['weight']]\n",
    "axes[0].barh(range(len(top_15)), top_15['weight'], color=colors)\n",
    "axes[0].set_yticks(range(len(top_15)))\n",
    "axes[0].set_yticklabels(top_15['feature'])\n",
    "axes[0].set_xlabel('Weight')\n",
    "axes[0].set_title('Top 15 Features by Importance')\n",
    "axes[0].axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "\n",
    "# Plot 2: Odds ratios\n",
    "axes[1].barh(range(len(top_15)), top_15['odds_ratio'], color=colors)\n",
    "axes[1].set_yticks(range(len(top_15)))\n",
    "axes[1].set_yticklabels(top_15['feature'])\n",
    "axes[1].set_xlabel('Odds Ratio')\n",
    "axes[1].set_title('Odds Ratios for Top Features')\n",
    "axes[1].axvline(x=1, color='black', linestyle='--', linewidth=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USING THE MODEL - PREPROCESSING NEW DATA\n",
    "# ============================================================================\n",
    "\n",
    "# New customer data (single customer)\n",
    "new_customer = {\n",
    "    'contract': 'month-to-month',\n",
    "    'tenure': 12,\n",
    "    'monthly_charges': 70.0,\n",
    "    'total_charges': 840.0,\n",
    "    'internet_service': 'fiber_optic',\n",
    "    'tech_support': 'no'\n",
    "}\n",
    "\n",
    "# IMPORTANT: Use transform(), NOT fit_transform()\n",
    "new_customer_encoded = dv.transform([new_customer])\n",
    "\n",
    "# For multiple customers\n",
    "new_customers = [\n",
    "    {'contract': 'month-to-month', 'tenure': 12},\n",
    "    {'contract': 'one-year', 'tenure': 24},\n",
    "    {'contract': 'two-year', 'tenure': 36}\n",
    "]\n",
    "new_customers_encoded = dv.transform(new_customers)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAKING PREDICTIONS ON NEW DATA\n",
    "# ============================================================================\n",
    "\n",
    "# Predict class label (0 or 1)\n",
    "prediction = model.predict(new_customer_encoded)\n",
    "print(f\"Prediction: {prediction[0]}\")\n",
    "\n",
    "# Interpret result\n",
    "if prediction[0] == 1:\n",
    "    print(\"Customer is likely to churn\")\n",
    "    print(\"Action: Initiate retention campaign\")\n",
    "else:\n",
    "    print(\"Customer is likely to stay\")\n",
    "    print(\"Action: Continue normal engagement\")\n",
    "\n",
    "# Get probability estimates\n",
    "probabilities = model.predict_proba(new_customer_encoded)\n",
    "prob_no_churn = probabilities[0][0]\n",
    "prob_churn = probabilities[0][1]\n",
    "\n",
    "print(f\"Probability of staying: {prob_no_churn:.2%}\")\n",
    "print(f\"Probability of churning: {prob_churn:.2%}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USING CUSTOM THRESHOLDS\n",
    "# ============================================================================\n",
    "\n",
    "# Get probability of churning\n",
    "prob_churn = model.predict_proba(new_customer_encoded)[0, 1]\n",
    "\n",
    "# Conservative approach - only flag very high risk\n",
    "threshold_conservative = 0.7\n",
    "if prob_churn >= threshold_conservative:\n",
    "    action = \"HIGH PRIORITY: Immediate intervention required\"\n",
    "elif prob_churn >= 0.5:\n",
    "    action = \"MEDIUM PRIORITY: Monitor and engage\"\n",
    "else:\n",
    "    action = \"LOW RISK: Standard engagement\"\n",
    "\n",
    "print(f\"Churn Probability: {prob_churn:.2%}\")\n",
    "print(f\"Action: {action}\")\n",
    "\n",
    "# Sensitive approach - catch more potential churners\n",
    "threshold_sensitive = 0.3\n",
    "if prob_churn >= threshold_sensitive:\n",
    "    action = \"Include in retention campaign\"\n",
    "else:\n",
    "    action = \"No action needed\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RISK-BASED SEGMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "# Predict probabilities for all customers\n",
    "all_customers_encoded = dv.transform(all_customers_dict)\n",
    "churn_probabilities = model.predict_proba(all_customers_encoded)[:, 1]\n",
    "\n",
    "# Create risk segments\n",
    "risk_segments = np.select(\n",
    "    [\n",
    "        churn_probabilities >= 0.7,\n",
    "        churn_probabilities >= 0.5,\n",
    "        churn_probabilities >= 0.3,\n",
    "        churn_probabilities < 0.3\n",
    "    ],\n",
    "    [\n",
    "        'Very High Risk',\n",
    "        'High Risk',\n",
    "        'Medium Risk',\n",
    "        'Low Risk'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create results dataframe\n",
    "results = pd.DataFrame({\n",
    "    'customer_id': customer_ids,\n",
    "    'churn_probability': churn_probabilities,\n",
    "    'risk_segment': risk_segments\n",
    "}).sort_values('churn_probability', ascending=False)\n",
    "\n",
    "print(results.head(20))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BATCH PREDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Predict for many customers at once\n",
    "customers_df = pd.read_csv('customers_to_score.csv')\n",
    "customers_dict = customers_df.to_dict(orient='records')\n",
    "\n",
    "# Encode\n",
    "customers_encoded = dv.transform(customers_dict)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(customers_encoded)\n",
    "probabilities = model.predict_proba(customers_encoded)[:, 1]\n",
    "\n",
    "# Add predictions to dataframe\n",
    "customers_df['churn_prediction'] = predictions\n",
    "customers_df['churn_probability'] = probabilities\n",
    "\n",
    "# Save results\n",
    "customers_df.to_csv('customer_predictions.csv', index=False)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"Total customers scored: {len(customers_df)}\")\n",
    "print(f\"Predicted churners: {predictions.sum()}\")\n",
    "print(f\"Average churn probability: {probabilities.mean():.2%}\")\n",
    "print(f\"\\nRisk Distribution:\")\n",
    "print(customers_df['churn_probability'].describe())\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# REAL-WORLD DEPLOYMENT EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "def predict_churn_risk(customer_data, model, vectorizer):\n",
    "    \"\"\"\n",
    "    Predict churn risk for a customer\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    customer_data : dict\n",
    "        Customer features as dictionary\n",
    "    model : LogisticRegression\n",
    "        Trained model\n",
    "    vectorizer : DictVectorizer\n",
    "        Fitted vectorizer\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Prediction results with probability and risk level\n",
    "    \"\"\"\n",
    "    # Encode customer data\n",
    "    customer_encoded = vectorizer.transform([customer_data])\n",
    "    \n",
    "    # Get prediction and probability\n",
    "    prediction = model.predict(customer_encoded)[0]\n",
    "    probability = model.predict_proba(customer_encoded)[0, 1]\n",
    "    \n",
    "    # Determine risk level\n",
    "    if probability >= 0.7:\n",
    "        risk_level = \"Very High\"\n",
    "        action = \"Immediate retention intervention required\"\n",
    "        priority = 1\n",
    "    elif probability >= 0.5:\n",
    "        risk_level = \"High\"\n",
    "        action = \"Proactive outreach recommended\"\n",
    "        priority = 2\n",
    "    elif probability >= 0.3:\n",
    "        risk_level = \"Medium\"\n",
    "        action = \"Include in next retention campaign\"\n",
    "        priority = 3\n",
    "    else:\n",
    "        risk_level = \"Low\"\n",
    "        action = \"Continue standard engagement\"\n",
    "        priority = 4\n",
    "    \n",
    "    return {\n",
    "        'will_churn': bool(prediction),\n",
    "        'churn_probability': float(probability),\n",
    "        'risk_level': risk_level,\n",
    "        'recommended_action': action,\n",
    "        'priority': priority\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "customer = {\n",
    "    'contract': 'month-to-month',\n",
    "    'tenure': 3,\n",
    "    'monthly_charges': 85.0,\n",
    "    'internet_service': 'fiber_optic',\n",
    "    'tech_support': 'no'\n",
    "}\n",
    "\n",
    "result = predict_churn_risk(customer, model, dv)\n",
    "print(f\"Churn Prediction: {result['will_churn']}\")\n",
    "print(f\"Risk Probability: {result['churn_probability']:.2%}\")\n",
    "print(f\"Risk Level: {result['risk_level']}\")\n",
    "print(f\"Recommended Action: {result['recommended_action']}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CREATING A SCORING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "class ChurnPredictionPipeline:\n",
    "    \"\"\"Complete pipeline for churn prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, model, vectorizer):\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "    \n",
    "    def preprocess(self, customer_data):\n",
    "        \"\"\"Preprocess customer data\"\"\"\n",
    "        # Add any data validation or cleaning here\n",
    "        return customer_data\n",
    "    \n",
    "    def predict(self, customer_data):\n",
    "        \"\"\"Make prediction for single customer\"\"\"\n",
    "        processed = self.preprocess(customer_data)\n",
    "        encoded = self.vectorizer.transform([processed])\n",
    "        prediction = self.model.predict(encoded)[0]\n",
    "        probability = self.model.predict_proba(encoded)[0, 1]\n",
    "        return prediction, probability\n",
    "    \n",
    "    def predict_batch(self, customers_list):\n",
    "        \"\"\"Make predictions for multiple customers\"\"\"\n",
    "        processed = [self.preprocess(c) for c in customers_list]\n",
    "        encoded = self.vectorizer.transform(processed)\n",
    "        predictions = self.model.predict(encoded)\n",
    "        probabilities = self.model.predict_proba(encoded)[:, 1]\n",
    "        return predictions, probabilities\n",
    "    \n",
    "    def get_risk_segment(self, probability):\n",
    "        \"\"\"Assign risk segment based on probability\"\"\"\n",
    "        if probability >= 0.7:\n",
    "            return 'Very High Risk'\n",
    "        elif probability >= 0.5:\n",
    "            return 'High Risk'\n",
    "        elif probability >= 0.3:\n",
    "            return 'Medium Risk'\n",
    "        else:\n",
    "            return 'Low Risk'\n",
    "    \n",
    "    def score_customer(self, customer_data):\n",
    "        \"\"\"Complete scoring with all details\"\"\"\n",
    "        prediction, probability = self.predict(customer_data)\n",
    "        risk_segment = self.get_risk_segment(probability)\n",
    "        \n",
    "        return {\n",
    "            'prediction': int(prediction),\n",
    "            'probability': float(probability),\n",
    "            'risk_segment': risk_segment,\n",
    "            'will_churn': bool(prediction)\n",
    "        }\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = ChurnPredictionPipeline(model, dv)\n",
    "\n",
    "# Score a customer\n",
    "customer = {'contract': 'month-to-month', 'tenure': 5}\n",
    "score = pipeline.score_customer(customer)\n",
    "print(score)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HANDLING EDGE CASES\n",
    "# ============================================================================\n",
    "\n",
    "def safe_predict(customer_data, model, vectorizer):\n",
    "    \"\"\"Prediction with error handling\"\"\"\n",
    "    try:\n",
    "        # Validate required fields\n",
    "        required_fields = ['contract', 'tenure', 'monthly_charges']\n",
    "        missing_fields = [f for f in required_fields if f not in customer_data]\n",
    "        \n",
    "        if missing_fields:\n",
    "            return {\n",
    "                'error': f\"Missing required fields: {missing_fields}\",\n",
    "                'prediction': None\n",
    "            }\n",
    "        \n",
    "        # Make prediction\n",
    "        encoded = vectorizer.transform([customer_data])\n",
    "        prediction = model.predict(encoded)[0]\n",
    "        probability = model.predict_proba(encoded)[0, 1]\n",
    "        \n",
    "        return {\n",
    "            'error': None,\n",
    "            'prediction': int(prediction),\n",
    "            'probability': float(probability)\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': f\"Prediction failed: {str(e)}\",\n",
    "            'prediction': None\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "result = safe_predict(customer, model, dv)\n",
    "if result['error']:\n",
    "    print(f\"Error: {result['error']}\")\n",
    "else:\n",
    "    print(f\"Prediction: {result['prediction']}\")\n",
    "    print(f\"Probability: {result['probability']:.2%}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MONITORING PREDICTIONS IN PRODUCTION\n",
    "# ============================================================================\n",
    "\n",
    "import datetime\n",
    "\n",
    "def log_prediction(customer_id, prediction, probability, timestamp=None):\n",
    "    \"\"\"Log predictions for monitoring\"\"\"\n",
    "    if timestamp is None:\n",
    "        timestamp = datetime.datetime.now()\n",
    "    \n",
    "    log_entry = {\n",
    "        'timestamp': timestamp,\n",
    "        'customer_id': customer_id,\n",
    "        'prediction': prediction,\n",
    "        'probability': probability,\n",
    "        'model_version': 'v1.0'\n",
    "    }\n",
    "    \n",
    "    # Save to database or file\n",
    "    # For demo, append to CSV\n",
    "    log_df = pd.DataFrame([log_entry])\n",
    "    log_df.to_csv('prediction_log.csv', mode='a', header=False, index=False)\n",
    "    \n",
    "    return log_entry\n",
    "\n",
    "# Make and log prediction\n",
    "customer_id = 'CUST_12345'\n",
    "customer_data = {}\n",
    "prediction, probability = pipeline.predict(customer_data)\n",
    "log_prediction(customer_id, prediction, probability)\n",
    "\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
