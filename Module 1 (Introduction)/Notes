# Introduction to Machine Learning - Comprehensive Notes

## 1. Introduction to Machine Learning

### What is Machine Learning?
Machine Learning (ML) is a technique that allows us to build models that extract patterns from data, similar to how human experts analyze information to make decisions. The essence of ML is: **given data, have a model learn patterns in the data**.

### Real-World Example: Used Car Price Prediction

**The Problem**: When selling a car online, determining the right price is crucial. Too high and it won't sell; too low and you lose money.

**How It Works**:
- **Input**: Car properties (age, mileage, model, number of doors, brand, condition)
- **Process**: Patterns can be found in historical car sales data
- **Output**: Recommended price based on learned patterns
- **User Experience**: User provides car information → System extracts data → Price predicted and suggested

**Traditional Expert Approach**: 
- Human expert examines car features and uses experience to identify market patterns
- Makes informed price estimate based on these patterns

**Machine Learning Approach**:
- Collect historical data on car sales with features and actual sale prices
- Train ML model to automatically identify the same patterns experts use
- Model predicts price for new cars based on learned patterns

### Essential Terminology

- **Features**: The characteristics or input variables we know about our data (car year, make, mileage, condition)
- **Target**: The variable we want to predict (car price, spam classification)
- **Model**: A "description of statistical patterns" that predicts a target given input features
- **Training**: The process where algorithms extract patterns from data and store them in the model

### The ML Process
1. **Training Phase**: Features + Target → Algorithm → Model (learns patterns)
2. **Prediction Phase**: New Features + Trained Model → Predictions

---

## 2. ML vs Traditional Programming vs Rule-Based Systems

### Programming Paradigm Shift

**Traditional Programming**:
```
Data + Code → Outcome
```

**Machine Learning**:
```
Data + Outcome → Model
New Data + Model → Prediction
```

### Detailed Example: Email Spam Detection System

**The Challenge**: Develop a spam detection classifier that can identify spam emails effectively.

#### Rule-Based System Approach

**Method**: Create explicit rules by analyzing spam messages to identify what makes them spam.

**Example Rules**:
- Filter by sender: "If sender contains 'promotions@' → mark as spam"
- Filter by domain: "If from suspicious domain → mark as spam"  
- Filter by keywords: "If contains 'money', 'funding', 'taxes' → mark as spam"
- Filter by patterns: "If subject line has excessive capitals → mark as spam"

**Major Limitations**:
- Spam messages constantly evolve and adapt
- Requires frequent manual updates to rules
- Creates a complex web of logic that becomes a maintenance nightmare
- Cannot keep up with rapidly changing spam techniques
- Human language complexity makes comprehensive rules impossible
- Becomes obsolete quickly and lacks acceptable effectiveness

#### Machine Learning Approach

**1. Getting the Data**:
- Implement "spam" button for users to mark emails
- Collect both spam and legitimate emails to create balanced dataset
- Use existing spam folders as labeled training data

**2. Define and Calculate Features**:
Start with rule-based insights as features:
- Length of subject > 10 characters: Yes/No
- Length of body > 10 characters: Yes/No  
- Sender from suspicious domain: Yes/No
- Sender email matches spam pattern: Yes/No
- Contains financial keywords: Yes/No

**Feature Vector Example**:
For a given email: `[1, 1, 0, 0, 1]`
- Subject length > 10: Yes (1)
- Body length > 10: Yes (1)  
- Suspicious domain: No (0)
- Spam email pattern: No (0)
- Financial keywords: Yes (1)

If user marked this email as spam, target = 1

**3. Train the Model**:
- Features matrix + Target vector → ML Algorithm
- Algorithm finds unknown coefficients that minimize prediction error
- Model learns patterns that distinguish spam from legitimate email

**4. Use the Model**:
- New email → Extract features → Model predicts probability of spam
- Decision threshold determines final classification

**Key Advantage**: ML doesn't discard all rule-based approaches. Rule-based features can be incorporated as ML features, combining domain expertise with learning capability.

---

## 3. Types of Machine Learning

The examples above belong to **Supervised Learning**, which relies on labeled data (known targets) to train models. Other types include:

- **Unsupervised Learning**: Finding patterns in data without known targets
- **Reinforcement Learning**: Learning through interaction and reward systems

*Note: This course focuses on Supervised Learning.*

---

## 4. Understanding Supervised Machine Learning

### Definition and Core Concept
In supervised ML, we provide both the data (features) and the target variable. The ML model learns patterns in this data, which can then be used to generalize to new, unseen samples.

### Mathematical Notation

**Data Structure**:
- **Feature Matrix (X)**: 2D array where rows = observations/samples, columns = features
- **Target Vector (y)**: Column vector containing target values for each observation
- **X is our input, y is our output**

```
   Features (X)          Target (y)
       f1  f2  f3  f4
obs 1:  1   1   0   1      1
obs 2:  1   0   0   1      0  
obs 3:  0   1   0   0      0
obs 4:  1   1   1   1      1
```

**Model Function**:
If we have a model g, then: **g(X) ≈ y**
The model approximates the target within some error margin.

### Types of Supervised Learning Problems

#### 4.1 Regression Problems
- **Purpose**: Predict continuous numerical values
- **Target**: Numbers (prices, temperatures, distances)
- **Model Output**: g(X) outputs a numerical value (e.g., car price)
- **Examples**: 
  - Car price prediction: $15,000, $22,500, etc.
  - House price forecasting
  - Stock market prediction

#### 4.2 Classification Problems
- **Purpose**: Predict categories or classes  
- **Target**: Discrete categories
- **Model Output**: g(X) outputs probability or category

**Binary Classification**:
- Two categories (1 or 0, spam/not spam, positive/negative)
- Model outputs: 1 = spam, 0 = not spam
- Examples: Medical diagnosis, fraud detection, email filtering

**Multiclass Classification**:
- Multiple categories (cat/dog/bird/horse, product categories)
- Model outputs one of many possible classes
- Examples: Image recognition, document categorization

#### 4.3 Ranking Problems
- **Purpose**: Order items by providing scores for each item
- **Applications**: Recommendation systems, search results
- **Note**: Can often be solved using regression or classification approaches
- **Examples**: Netflix recommendations, Google search rankings

---

## 5. CRISP-DM Methodology

### Overview
CRISP-DM (Cross-Industry Standard Process for Data Mining) is a methodology for organizing ML projects. It provides a structured approach: **understand the problem → collect the data → train the model → use it**.

**High-Level ML Project Flow**: 
```
Email → Model → Spam/Not Spam
```

### The 6 Phases of CRISP-DM

#### 5.1 Business Understanding
**Primary Goals**:
- Identify the problem to solve
- Understand if the problem is important and worth solving
- Determine how to measure success
- Decide whether ML is needed

**Key Questions**:
- How serious is the issue? (One user complaint vs company-wide problem)
- What is the extent of the problem?
- Will Machine Learning actually help?
- Can the problem be solved with simpler methods?

**Define Measurable Goals**:
- Must be specific and quantifiable
- **Example**: "Reduce amount of spam messages by 50%"
- **Alternative**: "Reduce spam-related user complaints by 30%"

**Spam Detection Example**:
- Problem: Users complain about spam
- Assessment: Is it one user or many users?
- Decision: Do we need ML or can we solve it differently?
- Goal: Reduce spam messages by 50%

#### 5.2 Data Understanding
**Primary Focus**: Ensure data availability and quality

**Key Activities**:
- Identify available data sources  
- Assess what data is missing and how to collect it
- Evaluate data quality and reliability
- Determine if dataset is large enough for training

**Spam Detection Example Questions**:
- Do we have a "spam" button implemented?
- Is the data behind this button reliable and good enough?
- Do we track spam correctly in our systems?
- Is our dataset large enough for effective training?
- Do we need to collect additional data?

**Feedback Loop**: Understanding data may reveal insights that influence the original business goal, requiring iteration back to Business Understanding.

#### 5.3 Data Preparation
**Objective**: Transform raw data into format suitable for ML algorithms

**Key Activities**:
- **Data Cleaning**: Remove noise, handle missing values, fix inconsistencies
- **Feature Extraction**: Identify and create relevant features
- **Pipeline Building**: Automate the transformation process
- **Format Conversion**: Convert to tabular form required by ML algorithms

**Output**: Clean, structured dataset ready for modeling

#### 5.4 Modeling
**Core Activity**: This is where the actual machine learning happens

**Process**:
- Try different models and algorithms
- Train multiple models on the prepared data
- Compare performance across different approaches
- Select the best performing model

**Model Options**:
- Logistic Regression
- Decision Trees  
- Neural Networks
- Random Forest
- Support Vector Machines
- And many others

**Iterative Nature**: May require returning to Data Preparation to:
- Fix issues with the data
- Add new features (feature engineering)
- Address problems discovered during modeling

#### 5.5 Evaluation
**Purpose**: Assess how well the model performs relative to business objectives

**Key Questions**:
- How well does the model solve the business problem?
- Have we reached our defined goals?
- Do our metrics show improvement?
- Did we solve/measure the right problem?
- Was the goal achievable and realistic?

**Decision Points Based on Results**:
- **Success**: Proceed to deployment
- **Partial Success**: Return to earlier phases for improvements
- **Failure**: Update goals or stop the project

**Retrospective Analysis**:
- What lessons were learned?
- What would we do differently next time?

#### 5.6 Deployment
**Modern Approach** (evolved from original CRISP-DM):
Often combined with Evaluation for continuous assessment

**Deployment Strategy**:
- **Gradual Rollout**: Deploy to small subset of users first (5-10%)
- **Online Evaluation**: Monitor real-world performance
- **A/B Testing**: Compare against current system
- **Full Deployment**: Roll out to all users if successful

**Ongoing Responsibilities**:
- Implement proper monitoring systems
- Ensure quality and maintainability  
- This represents the "engineering" phase of ML

### CRISP-DM Best Practices
- **Start Simple**: Go quickly through all steps with basic approach
- **Iterate Frequently**: Process is cyclical, expect multiple rounds
- **Learn from Each Iteration**: Use results to inform next cycle
- **Business-Driven**: Always tie technical work back to business objectives

---

## 6. Model Selection Process (The Modeling Step in Detail)

### The Challenge: Choosing the Best Model
The modeling step is where we try different models and choose the best one. This is where the actual ML happens.

### The Time-Based Problem
**Hypothetical Scenario**: 
- Train model in July using data X with output y
- Evaluate in August and find accuracy of 0.7
- **Ideal but Impossible**: Travel to future, get August data, and train with that

**Reality**: We can't predict the future, so we simulate it with data splitting.

### Basic Train-Validation Split

#### Data Splitting Strategy (80/20)
- **Training Data (80%)**: Represents "July" - used to train models
- **Validation Data (20%)**: Represents "August" - hidden during training, used for evaluation

#### Process:
1. **Training Phase**:
   - Use training data (X_train, y_train) to get model g
   
2. **Validation Phase**:
   - Validation data has features X_val and targets y_val
   - Use model to make predictions: ŷ_val = g(X_val)
   - Compare predictions ŷ_val with actual targets y_val
   - Calculate accuracy score

3. **Model Comparison**:
   - Repeat process for different algorithms (logistic regression, decision tree, neural network)
   - Compare accuracy scores to select best model

### The Multiple Comparisons Problem
**Definition**: When testing multiple models, one might appear significantly better just due to random luck in the data split, not because it's actually superior.

**Why This Happens**: ML models are probabilistic, so we need to guard against one model "getting lucky" with a particular data split.

**Risk**: The "lucky" model may perform poorly on new, unseen data.

### Advanced Three-Way Split Solution

#### Data Distribution (60/20/20):
- **Training Set (60%)**: Train multiple models
- **Validation Set (20%)**: Compare models and select the best
- **Test Set (20%)**: Final, unbiased evaluation (completely hidden until the end)

### Complete Model Selection Process

#### Detailed Step-by-Step Process:

1. **Data Split**: Divide dataset into train/validation/test sets

2. **Model Training**: 
   - Use (X_train, y_train) to train different models
   - Each algorithm learns different patterns

3. **Model Validation**:
   - For each trained model, use X_val to predict ŷ_val
   - Compare ŷ_val with y_val to calculate accuracy
   - Repeat for all models being compared

4. **Model Selection**: 
   - Select the model with best validation accuracy
   - This is our chosen model

5. **Final Testing**:
   - **Critical Step**: Use test data (X_test, y_test) only once
   - Make final predictions: ŷ_test = g(X_test)  
   - Compare ŷ_test with y_test

6. **Performance Verification**:
   - **Good Sign**: Test accuracy is similar to validation accuracy
   - **Red Flag**: Test accuracy is significantly different (model got lucky)
   - **Action**: If test results are poor, discard model and restart process

### Best Practices for Model Selection

**Data Usage Guidelines**:
- **Training data**: Used repeatedly for different models
- **Validation data**: Used repeatedly for model comparison
- **Test data**: Used exactly once for final evaluation

**Quality Assurance**:
- Test accuracy should align with validation accuracy
- Large discrepancies indicate unreliable model selection
- Multiple rounds may be needed to find truly robust model

**Optimization Strategy**:
- After model selection, consider retraining chosen model on combined (training + validation) data
- This maximizes data usage for the final model
- Only do this after test validation confirms model quality

---

## Key Takeaways

### Core Concepts
1. **ML automates pattern recognition** that human experts do manually
2. **Essence of ML**: Given data, have models learn patterns from that data  
3. **Paradigm shift**: From writing explicit rules to learning from data
4. **Supervised learning requires labeled training data** with known correct answers

### Practical Implementation
5. **Rule-based insights can become ML features** - don't discard domain expertise
6. **Feature engineering is crucial** - how you represent data affects model performance
7. **Data quality determines model quality** - garbage in, garbage out
8. **Start simple, then iterate** - don't over-engineer initial solutions

### Process and Methodology
9. **CRISP-DM provides structured approach** from business problem to deployment  
10. **Business understanding drives everything** - technical work must serve business goals
11. **Iteration is essential** - expect multiple cycles through all phases
12. **Data understanding influences goals** - be prepared to adjust objectives

### Model Development
13. **Proper data splitting prevents overfitting** and ensures reliable model selection
14. **Multiple comparisons problem is real** - guard against "lucky" models
15. **Test data is sacred** - use only once for final validation
16. **Validation vs test accuracy alignment** indicates model reliability

### Success Factors
- Clear, measurable business objectives
- High-quality, representative training data  
- Systematic evaluation and validation processes
- Gradual deployment with monitoring
- Continuous improvement based on real-world performance

# Model Selection Process

## The Challenge
• **We need to compare models:** How do you know which model works best?
• **We can't see the future:** We need to test models on data they haven't seen before
• **Solution:** Split your data to simulate "future" performance

## Data Splitting Strategy

### Basic Split (2-Way)
• **80% Training data:** Use this to teach the model
• **20% Validation data:** "Hide" this from training, use it to test performance

### How It Works
1. **Train the model:** Use training data (X, y) to create model `g`
2. **Make predictions:** Use model on validation data (Xv) to get predictions `ŷv`
3. **Check accuracy:** Compare predictions `ŷv` with actual answers `yv`
4. **Repeat for different models:** Try logistic regression, decision trees, neural networks, etc.
5. **Pick the best:** Choose model with highest accuracy score

### Concrete Example of Step 3
When comparing predictions to actual values:

| Prediction (Probability) | Prediction (Class) | Actual Target |
|-------------------------|-------------------|---------------|
| 0.8 | 1 | 1 |
| 0.7 | 1 | 0 |
| 0.6 | 1 | 1 |
| 0.1 | 0 | 0 |
| 0.9 | 1 | 1 |
| 0.6 | 1 | 0 |

**Result:** 4 out of 6 correct predictions = 66% accuracy

### The Problem: "Getting Lucky"
• **Multiple Comparisons Problem:** When you test many models, one might accidentally perform well just by chance
• **Like flipping coins:** If you flip 100 coins, some will get "lucky" streaks even if they're fair
• **In ML:** One model might get a favorable data split and look better than it really is

## Better Approach: 3-Way Split

### The Solution
• **60% Training:** Teach the models
• **20% Validation:** Compare models and pick the best one  
• **20% Test:** Final check to make sure the winner isn't just "lucky"

### The 6-Step Process
1. **Split data three ways:** Training/Validation/Test (60%-20%-20%)
2. **Train multiple models:** Use only training data
3. **Validate models:** Test each model on validation data
4. **Repeat steps 2-3:** Try different models and compare performance
5. **Select best model:** Pick the one with highest validation accuracy
6. **Final test:** Test the winner on test data (which it has never seen)
7. **Check results:** If test accuracy is similar to validation accuracy, your model is good!

### Model Comparison Example
| Model | Type | Validation Accuracy | Test Accuracy |
|-------|------|-------------------|---------------|
| g1 | Linear Regression | 66% | - |
| g2 | Decision Tree | 60% | - |
| g3 | Random Forest | 67% | - |
| g4 | Neural Network | 80% | 79% |

**Winner:** g4 (Neural Network) - similar validation and test performance shows it didn't just "get lucky"

### What If Test Results Are Very Different?
• **Bad news:** Your model probably got "lucky" during validation
• **Action needed:** Discard this model and try again
• **This protects you:** From deploying a model that won't work in real life

## Alternative Approach: Don't Waste Data

### Full Training Strategy
After model selection, you can maximize your data usage:

1. **Complete the 6-step process above** to select your best model
2. **Combine training + validation data:** Create a larger training dataset
3. **Retrain your winning model:** Use the combined dataset for final training
4. **Apply to test data:** Get your final performance assessment

### Why This Works
• **Don't waste the validation data:** Use it to make your final model even better
• **Larger training set:** Often leads to better model performance
• **Keep test data separate:** Still use it for unbiased final verification
• **Best of both worlds:** Rigorous model selection + maximum data utilization

### When to Use This Approach
• **Depends on your dataset:** May not always yield better results
• **Experiment carefully:** Compare both approaches on your specific problem
• **Dataset characteristics matter:** Effectiveness varies by data type and model performance

## Simple Example
Think of it like studying for an exam:
• **Training data = Study materials:** Learn from these
• **Validation data = Practice tests:** Compare how well you do on different study methods
• **Test data = Final exam:** The real test to see if you actually learned

## Key Takeaway
The key insight: You need to be honest about how well your model will work on completely new data it has never seen before. The 3-way split protects you from fooling yourself with a model that just got lucky.

**Remember:** Model selection is one of the most important processes in Machine Learning - it's your safeguard against overconfident, poorly-generalizing models.

### Creating Arrays with Zeros, Ones, or Constants

    import numpy as np 

    # zeros array
    zeros_array = np.zeros(10)
    print(zeros_array)

    # ones arrary
    ones_array = np.ones(10)
    print(ones_array)

    # constant array
    constant_array = np.full(10,3)
    print(constant_array)



